\subsection{Análisis de Pivoteo}

Para justificar que para el tipo de matriz banda no se produce pivoteo al aplicarse el algoritmo de Eliminación Gaussiana, analizamos propiedades varias de la matriz.
%No sabia que poner...


\subsubsection{A es diagonal dominante}
Queremos ver que A es diagonal dominante, es decir, que se cumple al siguiente inecuacion:\\
$$\mid \frac{1}{\Delta^2_r}-\frac{1}{r \Delta_r}\mid +
\mid \frac{1}{\Delta^2_r} \mid + 
\mid \frac{1}{r^2 \Delta^2_\theta} \mid +
\mid \frac{1}{r^2 \Delta^2_\theta} \mid
\leq \mid -\frac{2}{\Delta^2_r}+\frac{1}{r \Delta_r}-\frac{2}{r^2 \Delta^2_\theta} \mid$$  \\
Primero, nos deshacemos de los modulos. El primer termino es negativo solo cuando $r < {\Delta_r}$, pero esto no sucede debido a que $r= j \Delta_r + r_i$, con $j$ natural y $r_i$ real positivo. El resto de los terminos a la izquierda de la desigualdad tambien son positivos debido a que todas las viariables estan elevadas al cuadrado, por lo tanto es equivalente no tomar modulo para estos terminos. \\
Para el termino de la derecha, $-\frac{2}{r^2 \Delta^2_\theta}$ siempre es negativo, por lo que para que el termino sea positivo, $-\frac{2}{\Delta^2_r}+\frac{1}{r \Delta_r}$ debe de ser necesariamente positivo. Sin embargo, esto solo puede ocurrir cuando $r < \frac{\Delta_r}{2}$, por lo que siempre resulta negativo por el argumento anterior. De este modo, si multiplicamos este termino por -1, obtenemos un numero positivo. La inecuacion resultante es: \\ 
$$ \frac{1}{\Delta^2_r}-\frac{1}{r \Delta_r} +  
\frac{1}{\Delta^2_r} + 
\frac{1}{r^2 \Delta^2_\theta} +
\frac{1}{r^2 \Delta^2_\theta}
\leq \frac{2}{\Delta^2_r}-\frac{1}{r \Delta_r}+\frac{2}{r^2 \Delta^2_\theta}$$ \\

Sumamos los terminos y obtenemos: \\
$$\frac{2}{\Delta^2_r}-\frac{1}{r \Delta_r}+\frac{2}{r^2 \Delta^2_\theta} \leq \frac{2}{\Delta^2_r}-\frac{1}{r \Delta_r}+\frac{2}{r^2 \Delta^2_\theta}$$ \\
Podemos observar que se cumple la inecuacion como queriamos demostrar, y en particular, vale la igualdad.

\subsubsection{A es no singular}
Dada la matriz A diagonal dominante, veamos que A es no singular.
Por absurdo supongamos que es singular, esto es, $\exists x\neq 0$ tal que $x \in Nu(A)$.
Como el vector es distinto a cero, existe una coordenada que es mayor al resto. Llamemosla $x_{k}$.


Del sistema de ecuaciones Ax, tomemos la ecuacion k, que tiene esta pinta:                  

$$ a_{(k, 1)} * x_{1} + a_{(k, 2)} * x_{2} +... + a_{(k, n)} * x_{n} = 0 $$\\

Luego despejamos el término k y aplicamos módulo:\\

 $$ \mid a_{(k, k)} * x_{k} \mid  =  \left \arrowvert - \sum_{j=1,j\neq k}^{n}  a_{(k,j)} * x_{j} \right \arrowvert $$

 $$ \mid a_{(k, k)}\mid * \mid x_{k} \mid \leq \sum_{j=1,j\neq k}^{n} \mid a_{(k,j)}\mid * \mid x_{j} \mid $$


 $$ \mid a_{(k, k)}\mid  = \sum_{j=1,j\neq k}^{n} \mid a_{(k,j)}\mid *  \frac{\mid x_{j} \mid}{\mid x_{k} \mid}$$\\

Anteriormente probamos que A es diagonal dominante y que en particular vale la igualdad. Como cada elemento de la sumatoria (que vale exactamente $a_{(k, k)}$) es multiplicado por $\frac{\mid x_{j} \mid}{\mid x_{k} \mid}$ $\leq 1$, disminuyen su valor, con lo que la sumatoria se reduce y resulta:\\

$$\mid a_{(k,k)} \mid < \sum_{j=1, j\neq k}^{n}\mid a_{(k,j)} \mid $$\\

Lo que es absurdo, pues A era diagonal dominante. El absurdo provino de suponer que A es singular. Luego vale que A es no singular.